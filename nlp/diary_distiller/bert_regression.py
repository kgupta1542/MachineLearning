# -*- coding: utf-8 -*-
"""BERT Regression Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yxyTtgnI0O8ti2TLF53Wr9qpp4n54ICs

Written in collaboration with Boopalakrishnan Arul
"""
# Written in collaboration with Boopala Arul

import os
import io
import numpy as np

import tensorflow as tf

import tensorflow_hub as hub

!pip install tf-models-official
from official.modeling import tf_utils
from official import nlp
from official.nlp import bert
import official.nlp.optimization

"""data"""
import pandas as pd
from sklearn.metrics import pairwise
from sklearn.model_selection import train_test_split
import io

label_1 = "Happy/Sad"
label_2 = "satisfaction/dissatisfaction"

data = pd.read_csv(io.BytesIO('trainingset_v2.csv'),nrows=100) #100 rows
#print(data.head())
train, test = train_test_split(data, test_size=0.3, random_state=42, shuffle=True)
train_data = np.asarray(train["text"])
train_labels = np.transpose([np.asarray(train[label_1]), np.asarray(train[label_2])])
test_data = np.asarray(test["text"])
test_labels = np.transpose([np.asarray(test[label_1]), np.asarray(test[label_2])])

!pip install tensorflow_text
import tensorflow_text as text  # Registers the ops.

text_inputs = tf.keras.layers.Input(shape=(), dtype=tf.string)
preprocessing = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1")
encoder_inputs = preprocessing(text_inputs)

encoder = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3",
    trainable=False)
#print(f"The Hub encoder has {len(encoder.trainable_variables)} trainable variables")

dropout_inputs = encoder(encoder_inputs)

dropout = tf.keras.layers.Dropout(.2, input_shape=(None,768))
dense_inputs = dropout(dropout_inputs['pooled_output'])

#dense = tf.keras.layers.Dense(64, activation="relu")(dense_inputs)

outputs = tf.keras.layers.Dense(2)(dense_inputs)

model = tf.keras.Model(inputs=text_inputs, outputs=outputs, name="infer_emotions")
model.summary()
tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)

"""optimizer"""

epochs = 25
batch_size = 5
eval_batch_size = 10

"""train_data_size = len(train_labels)
steps_per_epoch = int(train_data_size / batch_size)
num_train_steps = steps_per_epoch * epochs
warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)"""

#rn it's adamw
optimizer = tf.keras.optimizers.Adam()

"""training"""

loss = tf.keras.losses.MeanAbsoluteError()

model.compile(
    optimizer=optimizer,
    loss=loss,
    metrics=['mse'])

history = model.fit(
      train_data, train_labels,
#      validation_data=(val_data, val_labels),
      batch_size=batch_size,
      epochs=epochs)

import matplotlib.pyplot as plt
print(history.history.keys())

plt.plot(history.history['loss'])
plt.plot(history.history['mse'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend('train', loc='upper left')
plt.show()

model.evaluate(test_data, test_labels);

predictArr = model.predict(test_data)
#print(predictArr[0:10])

happyCounter = 0
satCounter = 0
totCounter = 0

for i in range(len(predictArr)):
  hapBool = False
  satBool = False

  if abs(predictArr[i][0] - test_labels[i][0]) < 0.05:
    happyCounter += 1
    hapBool = True
  if abs(predictArr[i][1] - test_labels[i][1]) < 0.125:
    satCounter += 1
    satBool = True
  if hapBool and satBool:
    totCounter += 1

print(happyCounter/len(predictArr))
print(satCounter/len(predictArr))
print(totCounter/len(predictArr))

print(model.predict(test_data))
#print(test_labels[10:20])

"""additional testing"""

def send_query(text, query_length):
    split_raw = text.split() #split into individual words
    steps = range(0, len(split_raw), query_length) #start, stop, step
    query_raw = (split_raw[i : i+query_length] for i in steps) #generate sub-lists
    queries = " ".join(i) for i in query_raw] #each sub-list turned to string
    #x_test = bert_encode(queries, tokenizer)
    result = model.predict(queries)
    return tf.argmax(result, axis = 1).numpy()

result = send_query("Old Mcdondals thasf wer fadw fsadf we sdflkj", 100)
print(result)
#print([emotions[i] for i in result])

result = send_query("""By now it's a tired observation to say that Ryan Murphy's shows tend to start strong and fall apart somewhere in the middle, but 'Ratched' breaks all previous Murphy records by lasting exactly thirty minutes before Sarah Paulson as Mildred attempts to seduce a pantsless Corey Stoll with a breathy monologue about her childhood abandonment issues as a prelude to threatening a nurse's children and coercing a mentally ill man to slit his own throat. The over-the-top violence, nonsense plotting, and straight up extraterrestrial behavioral choices that usually taint the later episodes of Murphy's shows are all 'Ratched' has to offer from the start."""
                    , 20)

print(result)
print([emotions[i] for i in result])

"""saving"""
from google.colab import files

export_dir='./infer_emotion'

tf.saved_model.save(model, export_dir=export_dir)

!zip -r ./infer_emotion.zip ./infer_emotion

#files.download("./infer_emotion.zip")