# -*- coding: utf-8 -*-
"""Tensorflow Test #3 (Regression)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aidjSyLMOMABkyrnVxPb505S82C_NgAM
"""

import numpy as np

import tensorflow as tf
import tensorflow.keras.layers as layers
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow.keras.datasets.imdb as imdb

vocab_size = 50000
max_len = 1000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size, maxlen=(max_len+1))
print(len(x_train))
print(len(x_test))

x_train_padded = pad_sequences(x_train)
x_test_padded = pad_sequences(x_test)

model = tf.keras.Sequential([
      layers.Embedding(vocab_size, 100, input_length=max_len),
      layers.GlobalAveragePooling1D(), #Could replace this with a Bidirectional, but computing time is insane
      layers.Dense(64, activation='relu'),
      layers.Dropout(0.25),
      layers.Dense(1)
])

model.summary()
tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)

loss_fn = tf.keras.losses.MeanSquaredError()
model.compile(optimizer='adam', loss=loss_fn, metrics=['mae'])

epochs = 10
batch_size = 32

model.fit(x_train_padded, y_train, verbose=2, batch_size=batch_size, epochs=epochs)

model.evaluate(x_test_padded, y_test, verbose=2)

#print(np.expand_dims(x_test_padded[0], axis=0))
print(model.predict(x_test_padded[20:30]))
print(y_test[20:30])

predictArr = model.predict(x_train_padded)
counter = 0
for i in range(len(predictArr)):
  if abs(predictArr[i] - y_train[i]) < 0.2:
    counter += 1

print(counter)
print(counter/len(predictArr))
